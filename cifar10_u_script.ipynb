{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train the base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module=cifar10_u name=cifar10 dname=original checkpoint_to_continue= num_timesteps=1024 num_iters=10 batch_size=1 lr=5e-05 scheduler=StrategyConstantLR diffusion=GaussianDiffusionDefault log_interval=15 ckpt_interval=30 num_workers=4 k=1\n",
      "0 0 256 256 256\n",
      "0 1 256 256 256\n",
      "0 2 256 256 256\n",
      "1 0 256 256 256\n",
      "1 1 256 256 256\n",
      "1 2 256 256 256\n",
      "2 0 256 256 256\n",
      "2 1 256 256 256\n",
      "2 2 256 256 256\n",
      "0 0 256 256 256\n",
      "0 1 256 256 256\n",
      "0 2 256 256 256\n",
      "1 0 256 256 256\n",
      "1 1 256 256 256\n",
      "1 2 256 256 256\n",
      "2 0 256 256 256\n",
      "2 1 256 256 256\n",
      "2 2 256 256 256\n",
      "0 0 256 256 256\n",
      "0 1 256 256 256\n",
      "0 2 256 256 256\n",
      "1 0 256 256 256\n",
      "1 1 256 256 256\n",
      "1 2 256 256 256\n",
      "2 0 256 256 256\n",
      "2 1 256 256 256\n",
      "2 2 256 256 256\n",
      "Training new model...\n",
      "Training...\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/teamspace/studios/this_studio/./train.py\", line 102, in <module>\n",
      "    train_model(args, make_model, make_dataset)\n",
      "  File \"/teamspace/studios/this_studio/./train.py\", line 92, in train_model\n",
      "    diffusion_train.train(train_loader, teacher_diffusion, teacher_ema, args.lr, device, make_extra_args=make_condition, on_iter=on_iter)\n",
      "  File \"/teamspace/studios/this_studio/train_utils.py\", line 159, in train\n",
      "    loss = diffusion.p_loss(img, time, extra_args)\n",
      "  File \"/teamspace/studios/this_studio/diffusion.py\", line 108, in p_loss\n",
      "    eps_pred = self.inference(z.float(), t.float(), extra_args)\n",
      "  File \"/teamspace/studios/this_studio/diffusion.py\", line 99, in inference\n",
      "    return self.net_(x, t * self.time_scale, **extra_args)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/teamspace/studios/this_studio/Unets/unet_torch.py\", line 350, in forward\n",
      "    h = block(h, emb)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/teamspace/studios/this_studio/Unets/unet_torch.py\", line 82, in forward\n",
      "    h = self.swish(self.norm1(x))  \n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/normalization.py\", line 313, in forward\n",
      "    return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/functional.py\", line 2965, in group_norm\n",
      "    return torch.group_norm(\n",
      "RuntimeError: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [256] and input of shape [1, 512, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "!python ./train.py --module cifar10_u --name cifar10 --num_timesteps 1024 --dname original --batch_size 1 --num_workers 4 --num_iters 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Distillate to 512 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module=cifar10_u name=cifar10 dname=base_0 base_checkpoint=./checkpoints/cifar10/original/checkpoint.pt gamma=0 checkpoint_to_continue= num_timesteps=1024 num_iters=10000 batch_size=128 lr=5e-05 scheduler=StrategyLinearLR diffusion=GaussianDiffusionDefault log_interval=5 ckpt_interval=30 num_workers=4 k=3\n",
      "Num timesteps: 8192, time scale: 1.\n",
      "Teacher parameters copied.\n",
      "Distillation...\n",
      "Loss: 0.004144867317739261: 100%|███████████████| 79/79 [02:10<00:00,  1.66s/it]\n",
      "Saved.\n",
      "Captured steps: 2731\n",
      "torch.Size([3, 32, 32])\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_0 --base_checkpoint ./checkpoints/cifar10/original/checkpoint.pt --batch_size 128 --num_workers 4 --num_iters 10000 --log_interval 5 --k 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distillate to 256 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module=cifar10_u name=cifar10 dname=base_1 base_checkpoint=./checkpoints/cifar10/base_0/checkpoint.pt gamma=0 checkpoint_to_continue= num_timesteps=1024 num_iters=10000 batch_size=128 lr=5e-05 scheduler=StrategyLinearLR diffusion=GaussianDiffusionDefault log_interval=5 ckpt_interval=30 num_workers=4 k=3\n",
      "1\n",
      "Num timesteps: 2730, time scale: 3.\n",
      "Teacher parameters copied.\n",
      "Distillation...\n",
      "Loss: 0.005234053009893094: 100%|███████████████| 79/79 [02:10<00:00,  1.65s/it]\n",
      "Saved.\n",
      "Captured steps: 911\n",
      "torch.Size([3, 32, 32])\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_1 --base_checkpoint ./checkpoints/cifar10/base_0/checkpoint.pt --batch_size 128 --num_workers 4 --num_iters 10000 --log_interval 5 --k 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distillate to 128 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module=cifar10_u name=cifar10 dname=base_2 base_checkpoint=./checkpoints/cifar10/base_1/checkpoint.pt gamma=0 checkpoint_to_continue= num_timesteps=1024 num_iters=50 batch_size=1 lr=5e-05 scheduler=StrategyLinearLR diffusion=GaussianDiffusionDefault log_interval=5 ckpt_interval=30 num_workers=4 k=3\n",
      "Num timesteps: 113, time scale: 9.\n",
      "Teacher parameters copied.\n",
      "Distillation...\n",
      "Loss: 0.00012528953228638592: 100%|█████████████| 50/50 [00:14<00:00,  3.56it/s]\n",
      "Saved.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_2 --base_checkpoint ./checkpoints/cifar10/base_1/checkpoint.pt --batch_size 1 --num_workers 4 --num_iters 50 --log_interval 5 --k 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distillate to 64 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module=cifar10_u name=cifar10 dname=base_3 base_checkpoint=./checkpoints/cifar10/base_2/checkpoint.pt gamma=0 checkpoint_to_continue= num_timesteps=1024 num_iters=50 batch_size=1 lr=5e-05 scheduler=StrategyLinearLR diffusion=GaussianDiffusionDefault log_interval=5 ckpt_interval=30 num_workers=4 k=3\n",
      "Num timesteps: 37, time scale: 27.\n",
      "Teacher parameters copied.\n",
      "Distillation...\n",
      "Loss: 7.873918906042832e-05: 100%|██████████████| 50/50 [00:13<00:00,  3.58it/s]\n",
      "Saved.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_3 --base_checkpoint ./checkpoints/cifar10/base_2/checkpoint.pt --batch_size 1 --num_workers 4 --num_iters 50 --log_interval 5 --k 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distillate to 32 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_4 --base_checkpoint ./checkpoints/cifar10/base_3/checkpoint.pt --batch_size 1 --num_workers 4 --num_iters 10000 --log_interval 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distillate to 16 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_5 --base_checkpoint ./checkpoints/cifar10/base_4/checkpoint.pt --batch_size 1 --num_workers 4 --num_iters 10000 --log_interval 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distillate to 8 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ./distillate.py --module cifar10_u --diffusion GaussianDiffusionDefault --name cifar10 --dname base_6 --base_checkpoint ./checkpoints/cifar10/base_5/checkpoint.pt --batch_size 1 --num_workers 4 --num_iters 10000 --log_interval 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "8192it [02:12, 61.79it/s]\n",
      "Captured steps: 8193\n",
      "torch.Size([3, 32, 32])\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./sample.py --out_file ./images/cifar10_u_teacher_1.png --module cifar10_u --checkpoint ./checkpoints/cifar10/original/checkpoint.pt --batch_size 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "2730it [00:43, 62.61it/s]\n",
      "Captured steps: 2731\n",
      "torch.Size([3, 32, 32])\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./sample.py --out_file ./images/cifar10_u_6_clipped.png --module cifar10_u --checkpoint ./checkpoints/cifar10/base_0/checkpoint.pt --batch_size 1 --clipped_sampling True --clipping_value 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "1024it [00:16, 61.94it/s]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python ./sample.py --out_file ./images/cifar10_original_ts1.png --module cifar10_u --time_scale 1 --checkpoint ./checkpoints/cifar10/original/checkpoint.pt --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook cifar10_u_script.ipynb to script\n",
      "[NbConvertApp] Writing 3202 bytes to cifar10_u_script_converted.py\n"
     ]
    }
   ],
   "source": [
    "# export to script\n",
    "!jupyter nbconvert --to script cifar10_u_script.ipynb --output cifar10_u_script_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 72732931\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from Unets.unet_ddim import UNet\n",
    "\n",
    "#checkpoint = torch.load('./checkpoints/cifar10/original/checkpoint.pt')\n",
    "\n",
    "model = UNet( in_channel=3,\n",
    "        channel=128,\n",
    "        channel_multiplier=[1, 2, 2, 4],\n",
    "        n_res_blocks=2,\n",
    "        attn_strides=[8],\n",
    "    )\n",
    "\n",
    "\n",
    "#model.load_state_dict(checkpoint[\"G\"])\n",
    "#model.load_state_dict(checkpoint) # Adjust key if needed\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#unofficial ddim vaala\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUnets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet_jax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UNet\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,  \u001b[38;5;66;03m# Set to 6 for 6 output channels\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m]\n",
      "File \u001b[0;32m~/Unets/unet_jax.py:407\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Remove this line: from Unets.unet_jax import UNet\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Instead, use the UNet class from the artifact I provided above\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_res_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m model\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m]\n",
      "File \u001b[0;32m~/Unets/unet_jax.py:259\u001b[0m, in \u001b[0;36mUNet.__init__\u001b[0;34m(self, num_classes, ch, emb_ch, out_ch, ch_mult, num_res_blocks, attn_resolutions, num_heads, head_dim, dropout, logsnr_input_type, logsnr_scale_range, resblock_resample, in_channel, channel, channel_multiplier, n_res_blocks, attn_strides, k)\u001b[0m\n\u001b[1;32m    256\u001b[0m level_blocks\u001b[38;5;241m.\u001b[39mappend(ResidualBlock(in_ch, out_ch_level, dropout\u001b[38;5;241m=\u001b[39mdropout))\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_ch_level \u001b[38;5;129;01min\u001b[39;00m attn_resolutions:\n\u001b[0;32m--> 259\u001b[0m     level_attns\u001b[38;5;241m.\u001b[39mappend(\u001b[43mAttentionBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_ch_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     level_attns\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mIdentity())\n",
      "File \u001b[0;32m~/Unets/unet_jax.py:132\u001b[0m, in \u001b[0;36mAttentionBlock.__init__\u001b[0;34m(self, resample, num_heads, head_dim, in_ch)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     head_dim \u001b[38;5;241m=\u001b[39m \u001b[43min_ch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     num_heads \u001b[38;5;241m=\u001b[39m in_ch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m head_dim\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "#unofficial ddim vaala\n",
    "import torch\n",
    "from Unets.unet_jax import UNet\n",
    "\n",
    "model = UNet(\n",
    "\n",
    "    num_classes=1,\n",
    "    ch=256,\n",
    "    emb_ch=1024,\n",
    "    out_ch=3,  # e.g., RGB output\n",
    "    ch_mult=(1, 1, 1),\n",
    "    num_res_blocks=3,\n",
    "    attn_resolutions=(8, 16),\n",
    "    num_heads=1,\n",
    "    dropout=0.2,\n",
    "    logsnr_input_type=\"inv_cos\",\n",
    "    resblock_resample=True,\n",
    "    k=6,  # Set to 6 for 6 output channels\n",
    ")\n",
    "model.image_size = [1, 3, 32, 32]\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 290759171\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from Unets.unet_final import UNet_final\n",
    "\n",
    "model = UNet_final(\n",
    "    in_channel=3,\n",
    "        channel=256,\n",
    "        channel_multiplier=[1, 2, 2, 4],\n",
    "        n_res_blocks=2,\n",
    "        attn_strides=[8],\n",
    "        k=1,)\n",
    "total_params = sum(p.numel() for p in model.parameters() )\n",
    "print(f\"Total trainable parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 72732931\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Unets.unet_ddim import UNet\n",
    "\n",
    "model = UNet(\n",
    "    in_channel=3,\n",
    "        channel=128,\n",
    "        channel_multiplier=[1, 2, 2, 4],\n",
    "        n_res_blocks=2,\n",
    "        attn_strides=[8],\n",
    "        k=1,)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
